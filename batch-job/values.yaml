# Default values for batch-job.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: gitlab.planetrover.ca:5050/sequoiadp/batch-job
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: "batchjob-dev"

imagePullSecrets: [name: docker-login]
nameOverride: ""
fullnameOverride: ""

sparkJob:
  namespace: "batchjob"
  sparkVersion: "3.1.2"
  image: "gitlab.planetrover.ca:5050/sequoiadp/spark:423195d2aa00748f519db1fd7f993478316bb69d" #"gcr.io/spark-operator/spark:v3.1.1"
  # list of strings
  imagePullSecrets: [name: docker-login]
  imagePullPolicy: IfNotPresent
  serviceAccount: spark
  restartPolicy:
    type: Never
  sparkConf:
    spark.jars.ivy:                                          /tmp/.ivy
    spark.sql.extensions:                                    io.delta.sql.DeltaSparkSessionExtension
    spark.sql.catalog.spark_catalog:                         org.apache.spark.sql.delta.catalog.DeltaCatalog
    spark.delta.logStore.class:                              org.apache.spark.sql.delta.storage.LocalLogStore
    spark.sql.warehouse.dir:                                 "s3a://staging-2-bucket/spark-warehouse/"
    spark.eventLog.enabled:                                  "true"
    spark.eventLog.dir:                                      "s3a://staging-2-bucket/spark-logs/"
    spark.hadoop.fs.s3a.endpoint:                            192.168.6.29:8002
    spark.hadoop.fs.s3a.access.key:                          ABCDEFGHIJKLMNOPQRST
    spark.hadoop.fs.s3a.secret.key:                          abcdefghijklmnopqrstuvwxyz0123456789ABCD
    spark.hadoop.fs.s3a.impl:                                org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.connection.ssl.enabled:              false
    spark.kubernetes.container.image.pullPolicy:             IfNotPresent
    spark.hadoop.hive.metastore.uris:                        #thrift://schema:9083
    spark.sql.sequoiadp.metaservice.uri:                     #metadata:9090
    spark.history.fs.logDirectory:                           "s3a://staging-2-bucket/spark-logs/"
    spark.kubernetes.file.upload.path:                       /opt/spark/
    spark.driver.extraClassPath:                             /opt/spark/jars/spark-sequoiadb_3.0-3.2.0-SNAPSHOT.jar,/opt/spark/jars/sequoiadb-driver-5.0.2.jar,/opt/spark/jars/hadoop-aws-3.2.0.jar,/spark/jars/aws-java-sdk-bundle-1.11.375.jar
    spark.executor.extraClassPath:                           /opt/spark/jars/spark-sequoiadb_3.0-3.2.0-SNAPSHOT.jar,/opt/spark/jars/sequoiadb-driver-5.0.2.jar,/opt/spark/jars/hadoop-aws-3.2.0.jar,/spark/jars/aws-java-sdk-bundle-1.11.375.jar
    log4j.logger.org.apache.hadoop.metrics2:                 WARN
    spark.sql.session.timeZone:                              #"EST5EDT"
    spark.driver.extraJavaOptions: -Duser.timezone=Canada/Eastern
    spark.executor.extraJavaOptions: -Duser.timezone=Canada/Eastern
  driver:
    javaOptions: #-Duser.timezone=Canada/Eastern #Asia/Hong_Kong #EST5EDT #-Dlog4j.configuration=/opt/log4j.properties #-Dalluxio.master.rpc.addresses=alluxio-master-0:19998
  executor:
    javaOptions: #-Duser.timezone=Canada/Eastern #Asia/Hong_Kong #EST5EDT #-Dlog4j.configuration=/opt/log4j.properties #-Dalluxio.master.rpc.addresses=alluxio-master-0:19998
  sparkEventLogs:
    ss3BucketName: staging-2-bucket
    sparkEventLogDir: spark-logs
  bucketName: staging-2-bucket
  endpoint: 192.168.6.29:8002
  accessKey: ABCDEFGHIJKLMNOPQRST
  secretKey: abcdefghijklmnopqrstuvwxyz0123456789ABCD

s3:
  bucketName: staging-2-bucket
  endpoint: 192.168.6.29:8002
  accessKey: ABCDEFGHIJKLMNOPQRST
  secretKey: abcdefghijklmnopqrstuvwxyz0123456789ABCD
  secret: sequoiadp-services-s3-secret #{accessKey: ABCDEFGHIJKLMNOPQRST, secretKey: abcdefghijklmnopqrstuvwxyz0123456789ABCD}
          
sparkEventLogDir: "spark-logs"


rbac:
  create: true

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "batch-job"

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 8888

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
